{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a571a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35bf1c",
   "metadata": {},
   "source": [
    "### Links collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d5a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiParser():\n",
    "    def __init__(self,\n",
    "                 start_url='https://pa.wikipedia.org/w/index.php?title=ਖ਼ਾਸ:ਸ਼੍ਰੇਣੀਆਂ&dir=prev&offset=19ਵੀਂ_ਸਦੀ_ਵਿੱਚ_ਆਇਰਲੈਂਡ&limit=500',\n",
    "                 domain = 'https://pa.wikipedia.org',\n",
    "                 out_file = 'wiki_links_pa.txt'):\n",
    "        self.domain = domain\n",
    "        self.page_links = set()\n",
    "        self.out_file = out_file\n",
    "        self.start_url = start_url\n",
    "        self.current_chain = []\n",
    "        self.cycle = False\n",
    "        \n",
    "    def get_html_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.text if response.ok else None\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def get_next_link(self, soup):\n",
    "        try:\n",
    "            link = soup.find('div', {'class': 'mw-pager-navigation-bar'}).find('a', {'class': 'mw-nextlink'})['href']\n",
    "            return self.domain + link\n",
    "        except TypeError as e:\n",
    "            return None\n",
    "    \n",
    "    def exctract_article_links(self, soup):\n",
    "        pages = soup.find('div', {'id': 'mw-pages'})\n",
    "        if not pages:\n",
    "            return set()\n",
    "        pages = pages.find_all('li')\n",
    "        pages = [self.domain + page.find('a')['href'] for page in pages]\n",
    "        self.cycle = False\n",
    "        self.current_chain = []\n",
    "        return set(pages)\n",
    "        \n",
    "    \n",
    "    def extract_batch_articles(self, url):\n",
    "        print(url)\n",
    "        self.current_chain.append(url)\n",
    "        html = self.get_html_page(url)\n",
    "        if not html: return\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        article_links = self.exctract_article_links(soup)\n",
    "        \n",
    "        article_links = article_links.difference(self.page_links)\n",
    "        self.save_new_links_batch(article_links)\n",
    "        self.page_links = self.page_links.union(article_links)\n",
    "        self.log()\n",
    "        subcategories = soup.find('div', {'id': 'mw-subcategories'})\n",
    "\n",
    "        if subcategories:\n",
    "            for subcategory in subcategories.find_all('div', {'class': 'mw-category-group'}): \n",
    "                for tag in subcategory.find_all('a'):\n",
    "                    if self.current_chain:\n",
    "                        if max(Counter(self.current_chain).values()) > 1:\n",
    "                            self.cycle = True\n",
    "                            break\n",
    "                        self.extract_batch_articles(self.domain + tag['href'])\n",
    "                if self.cycle:\n",
    "                    self.cycle = False\n",
    "                    self.current_chain = []\n",
    "                    break\n",
    "        else: pass\n",
    "    \n",
    "    def get_all_category_links(self, url):\n",
    "        html = self.get_html_page(url)\n",
    "        soup = BeautifulSoup(html)\n",
    "        category_links = []\n",
    "        tags = soup.find('div', {'class': 'mw-spcontent'}).find_all('li')\n",
    "\n",
    "        # Find all valid categories\n",
    "        for tag in tags:\n",
    "            link = tag.find('a')['href']\n",
    "\n",
    "            if tag.text.find('(0 ਮੈਂਬਰ)') != -1 or link.find('redlink') != -1:\n",
    "                continue\n",
    "            \n",
    "            category_links.append(self.domain + link)\n",
    "\n",
    "        return category_links        \n",
    "        \n",
    "    \n",
    "    def get_all_page_links(self):\n",
    "        url = self.start_url\n",
    "        while True:\n",
    "            # Here we get the page with categories\n",
    "            html = self.get_html_page(url)\n",
    "            soup = BeautifulSoup(html)\n",
    "            \n",
    "            # Here we get all links from current category\n",
    "            category_links = self.get_all_category_links(url)\n",
    "\n",
    "            # Loop through category links and adding urls to articles\n",
    "            for link in category_links:\n",
    "                title = link.split('/')[-1]\n",
    "                self.save_current_category(title)\n",
    "                self.extract_batch_articles(link)\n",
    "            \n",
    "            # Here we get link to next page with categories\n",
    "            next_url = self.get_next_link(soup)\n",
    "            if not next_url: break\n",
    "            \n",
    "            # Go to next page\n",
    "            url = next_url\n",
    "    \n",
    "    def save_current_category(self, title):\n",
    "        with open(self.out_file, mode='a') as f:\n",
    "            f.write('{{{' + title + '}}}\\n')\n",
    "                       \n",
    "    def save_new_links_batch(self, links_batch):\n",
    "        links_batch = list(links_batch)\n",
    "        with open(self.out_file, mode='a') as f:\n",
    "            for link in links_batch:\n",
    "                f.write(link + '\\n')\n",
    "    \n",
    "    def log(self):\n",
    "        clear_output(wait=True)\n",
    "        print('Total num of links:', len(self.page_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4da2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of links: 2\n",
      "https://pa.wikipedia.org/wiki/%E0%A8%B8%E0%A8%BC%E0%A9%8D%E0%A8%B0%E0%A9%87%E0%A8%A3%E0%A9%80:%E0%A8%AE%E0%A9%8C%E0%A8%A4_1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create WikiParser object and parse all punjabi article links\n",
    "Parser = WikiParser()\n",
    "Parser.get_all_page_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b09668",
   "metadata": {},
   "source": [
    "### Article`s text collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8824b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticlesCollection():\n",
    "    def __init__(self, page_links=[], filename='./wiki_links_pa.txt'):\n",
    "        self.page_links = []\n",
    "        self.filename = filename\n",
    "        self.dirty_corpus = []\n",
    "    \n",
    "    def get_html_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.text if response.ok else None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def make_list_from_file(self):\n",
    "        with open(self.filename, mode='r') as f:\n",
    "            while True:\n",
    "                line = f.readline()[:-1]\n",
    "                if not line: break\n",
    "                if line.find('{{{') == -1: \n",
    "                    self.page_links.append(line)\n",
    "    \n",
    "    def make_raw_corpus(self, num_links=None):\n",
    "        links_processed = self.page_links[:num_links] if num_links else self.page_links\n",
    "        for link in tqdm(links_processed):\n",
    "            html = self.get_html_page(link)\n",
    "            if not html: continue\n",
    "            soup = BeautifulSoup(html)\n",
    "            \n",
    "            title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "            tags = soup.find('div', {'id': 'bodyContent', 'class': 'vector-body'}).find_all('p')\n",
    "            \n",
    "            text = [tag.text for tag in tags]\n",
    "            text = ' '.join(text)\n",
    "            text = '<>'.join((title, text))\n",
    "            text = '<>'.join((link, text))\n",
    "\n",
    "            self.dirty_corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837898de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026596546173095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 36852,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1353dd1eb63480ab5ad55fa016e8ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PagesCollector = ArticlesCollection()\n",
    "\n",
    "PagesCollector.make_list_from_file()\n",
    "PagesCollector.make_raw_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378a030",
   "metadata": {},
   "source": [
    "### Saving raw texts with \"^^^^^^\" separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef18657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dirty_corp_wiki_pa.txt', 'w') as f:\n",
    "    for line in PagesCollector.dirty_corpus:\n",
    "        f.write(line)\n",
    "        f.write('^^^^^^')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
